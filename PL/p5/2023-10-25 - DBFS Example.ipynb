{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6482be4c-f067-47c9-b0ac-35c938b94601",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# File location and type\n",
    "file_location = \"/FileStore/tables/STAR.csv\"\n",
    "file_type = \"csv\"\n",
    "\n",
    "# CSV options\n",
    "infer_schema = \"true\"\n",
    "first_row_is_header = \"true\"\n",
    "delimiter = \",\"\n",
    "\n",
    "# The applied options are for CSV files. For other file types, these will be ignored.\n",
    "df = spark.read.format(file_type) \\\n",
    "  .option(\"inferSchema\", infer_schema) \\\n",
    "  .option(\"header\", first_row_is_header) \\\n",
    "  .option(\"sep\", delimiter) \\\n",
    "  .load(file_location)\n",
    "\n",
    "display(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "50f87d27-d645-4637-8e00-7c318484e262",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import DoubleType\n",
    "df = df.withColumn(\"readk\",df[\"readk\"].cast(DoubleType()))\n",
    "df = df.withColumn(\"read1\",df[\"read1\"].cast(DoubleType()))\n",
    "df = df.withColumn(\"read2\",df[\"read2\"].cast(DoubleType()))\n",
    "df = df.withColumn(\"read3\",df[\"read3\"].cast(DoubleType()))\n",
    "df = df.withColumn(\"mathk\",df[\"mathk\"].cast(DoubleType()))\n",
    "df = df.withColumn(\"math1\",df[\"math1\"].cast(DoubleType()))\n",
    "df = df.withColumn(\"math2\",df[\"math2\"].cast(DoubleType()))\n",
    "df = df.withColumn(\"math3\",df[\"math3\"].cast(DoubleType()))\n",
    "df = df.withColumn(\"experiencek\",df[\"experiencek\"].cast(DoubleType()))\n",
    "df = df.withColumn(\"experience1\",df[\"experience1\"].cast(DoubleType()))\n",
    "df = df.withColumn(\"experience2\",df[\"experience2\"].cast(DoubleType()))\n",
    "df = df.withColumn(\"experience3\",df[\"experience3\"].cast(DoubleType()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a281ff58-bbe5-4e43-9366-bb26433ba7e9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.fs.rm('/FileStore/parquet/star', True)\n",
    "df.write.save('/FileStore/parquet/star', format='delta')\n",
    "df = spark.read.load('/FileStore/parquet/star')\n",
    "display(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c6dac167-4d09-4d74-a6f6-81078768df42",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import StringIndexer,OneHotEncoder\n",
    "colk = ['gender','ethnicity','birth','stark','star1','star2','star3','lunchk','lunch1','lunch2','lunch3','schoolk','school1','school2','school3','degreek','degree1','degree2','degree3','ladderk','ladder1','ladder2','ladder3','tethnicityk','tethnicity1','tethnicity2','tethnicity3','systemk','system1','system2','system3','schoolidk','schoolid1','schoolid2','schoolid3']\n",
    "colkI = [a+'I' for a in colk]\n",
    "colkE = [a+'E' for a in colk]\n",
    "indexer = StringIndexer(inputCols=colk, outputCols=colkI)\n",
    "encoder = OneHotEncoder(inputCols=colkI, outputCols=colkE)\n",
    "pipeline = Pipeline(stages=[indexer,encoder])\n",
    "df = pipeline.fit(df).transform(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "76d743ca-b8a7-433a-b0b7-35bf9ef6c670",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2e8435d1-3f4b-457b-9721-ca650787265a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "colinputk = ['genderI','ethnicityI','birthI','starkI','lunchkI','schoolkI','degreekI','ladderkI','tethnicitykI']\n",
    "\n",
    "# Dividir con randomSplit en 75% y 25%\n",
    "train, test = df.randomSplit([0.75, 0.25])\n",
    "\n",
    "# Crear un pipeline con tres etapas y aplicársela al conjunto de entrenamiento\n",
    "\n",
    "# Etapa 1. Con pyspark.ml.feature.Imputer, rellena losv alores perdidos de la\n",
    "#          variable de salida (strategy=\"mean\") en otra variable.\n",
    "from pyspark.ml.feature import Imputer\n",
    "im = Imputer(inputCol=\"readk\", outputCol=\"readk_imputado\", strategy=\"mean\")\n",
    "\n",
    "# Etapa 2. Con pyspark.ml.feature.VectorAssembler, combina todas las variables\n",
    "#          de entrada en una columna llamada \"myfeatures\".\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "assembler = VectorAssembler(inputCols=colinputk, outputCol=\"myfeatures\")\n",
    "\n",
    "# Etapa 3. Con pyspark.ml.regression.LinearRegression, ajusta un modelo a las\n",
    "#          variables de entrada \"myfeatures\" y de salida \"myreadk_imputado\".\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "lr = LinearRegression(featuresCol=\"myfeatures\", labelCol=\"readk_imputado\")\n",
    "\n",
    "# Crear un pipeline con las tres etapas anteriores\n",
    "df = Pipeline(stages=[im, assembler, lr]).fit(train).transform(train)\n",
    "\n",
    "# Por último, calcula el error cuadrático medio y R2 en conjunto de test.\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "evaluator = RegressionEvaluator(labelCol=\"readk_imputado\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "rmse = evaluator.evaluate(test)\n",
    "print(\"RMSE = %g\" % rmse)\n",
    "evaluator = RegressionEvaluator(labelCol=\"readk_imputado\", predictionCol=\"prediction\", metricName=\"r2\")\n",
    "r2 = evaluator.evaluate(test)\n",
    "print(\"R2 = %g\" % r2)\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "2023-10-25 - DBFS Example",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
